{
    "Import TensorFlow": {
        "prefix": "tf:import",
        "body": [
            "import tensorflow as tf"
        ],
        "description": "Import TensorFlow package"
	},
    "2D Convolution": {
        "prefix": "tf:conv2d",
        "body": [
            "${1:net} = tf.layers.conv2d(",
            "\t${1:net},",
            "\tfilters=${2:64},",
			"\tkernel_size=${3:3},",
            "\tpadding=\"${4:same}\"",
            "\tname=\"conv2d/${5:1}\")"
		],
        "description": "2D Convolution"
	},
    "Dense layer": {
        "prefix": "tf:dense",
        "body": [
            "${1:net} = tf.layers.dense(${1:net}, filters=${2:64}, name=\"dense/${3:1}\")"
		],
        "description": "Dense layer"
	},
    "2D max pooling layer": {
        "prefix": "tf:maxpool2d",
        "body": [
            "${1:net} = tf.layers.max_pooling2d(",
            "\t${1:net}, pool_size=${2:2}, strides=${2:2}, padding=\"${3:same}\")"
		],
        "description": "2D max pooling layer"
	},
    "Dropout layer": {
        "prefix": "tf:dropout",
        "body": [
            "${1:net} = tf.layers.dropout(${1:net}, rate=${2:0.5})"
		],
        "description": "Dropout layer"
	},
    "Softmax cross entropy loss": {
        "prefix": "tf:cross-entropy",
        "body": [
			"loss = tf.losses.sparse_softmax_cross_entropy(",
			"\tlabels=${1:labels}, logits=${2:logits})"
		],
        "description": "Softmax cross entropy loss"
	},
	"TensorFlow trainer": {
		"prefix": "tf:trainer",
		"body": [
			"from __future__ import absolute_import",
			"from __future__ import division",
			"from __future__ import print_function",
			"",
			"import tensorflow as tf",
			"",
			"tf.logging.set_verbosity(tf.logging.INFO)",
			"",
			"tf.flags.DEFINE_string(\"model\", \"\", \"Model name.\")",
			"tf.flags.DEFINE_string(\"dataset\", \"\", \"Dataset name.\")",
			"tf.flags.DEFINE_string(\"output_dir\", \"\", \"Optional output dir.\")",
			"tf.flags.DEFINE_string(\"schedule\", \"train_and_evaluate\", \"Schedule.\")",
			"tf.flags.DEFINE_string(\"hparams\", \"\", \"Hyper parameters.\")",
			"tf.flags.DEFINE_integer(\"num_epochs\", 100000, \"Number of training epochs.\")",
			"tf.flags.DEFINE_integer(\"save_summary_steps\", 10, \"Summary steps.\")",
			"tf.flags.DEFINE_integer(\"save_checkpoints_steps\", 10, \"Checkpoint steps.\")",
			"tf.flags.DEFINE_integer(\"eval_steps\", None, \"Number of eval steps.\")",
			"tf.flags.DEFINE_integer(\"eval_frequency\", 10, \"Eval frequency.\")",
			"",
			"FLAGS = tf.flags.FLAGS",
			"learn = tf.contrib.learn",
			"",
			"MODELS = {",
			"\t# This is a dictionary of models, the keys are model names, and the values",
			"\t# are the module containing HPARAMS, model_fn, and eval_fn.",
			"\t# Example: \"cnn\": cnn",
			"}",
			"",
			"DATASETS = {",
			"\t# This is a dictionary of datasets, the keys are dataset names, and the",
			"\t# values are the module containing HPARAMS, prepare_fn, read_fn, and",
			"\t# parser_fn. Example: \"mnist\": mnist",
			"}",
			"",
			"HPARAMS = {",
			"\t\"optimizer\": \"Adam\",",
			"\t\"learning_rate\": 0.001,",
			"\t\"decay_steps\": 10000,",
			"\t\"batch_size\": 128",
			"}",
			"",
			"def get_hparams():",
			"\t\"\"\"Aggregates and returns hyper parameters.\"\"\"",
			"\thparams = HPARAMS",
			"\thparams.update(DATASETS[FLAGS.dataset].HPARAMS)",
			"\thparams.update(MODELS[FLAGS.model].HPARAMS)",
			"",
			"\thparams = tf.contrib.training.HParams(**hparams)",
			"\thparams.parse(FLAGS.hparams)",
			"",
			"\treturn hparams",
			"",
			"",
			"def make_input_fn(mode, params):",
			"\t\"\"\"Returns an input function to read the dataset.\"\"\"",
			"\tdef _input_fn():",
			"\t\tdataset = DATASETS[FLAGS.dataset].read_fn(mode)",
			"\t\tif mode == learn.ModeKeys.TRAIN:",
			"\t\t\tdataset = dataset.repeat(FLAGS.num_epochs)",
			"\t\t\tdataset = dataset.shuffle(params.batch_size * 5)",
			"\t\tdataset = dataset.map(",
			"\t\t\tDATASETS[FLAGS.dataset].parser_fn, num_threads=8)",
			"\t\tdataset = dataset.batch(params.batch_size)",
			"\t\titerator = dataset.make_one_shot_iterator()",
			"\t\tfeatures, labels = iterator.get_next()",
			"\t\treturn features, labels",
			"\treturn _input_fn",
			"",
			"",
			"def make_model_fn():",
			"  \"\"\"Returns a model function.\"\"\"",
			"  def _model_fn(features, labels, mode, params):",
			"\tmodel_fn = MODELS[FLAGS.model].model_fn",
			"\tglobal_step = tf.train.get_or_create_global_step()",
			"\tpredictions, loss = model_fn(features, labels, mode, params)",
			"",
			"\ttrain_op = None",
			"\tif mode == learn.ModeKeys.TRAIN:",
			"\t\tdef decay(learning_rate, global_step):",
			"\t\t\tlearning_rate = tf.train.exponential_decay(",
			"\t\t\t\tlearning_rate, global_step, params.decay_steps, 0.5,",
			"\t\t\t\tstaircase=True)",
			"\t\t\treturn learning_rate",
			"",
			"\t\ttrain_op = tf.contrib.layers.optimize_loss(",
			"\t\t\tloss=loss,",
			"\t\t\tglobal_step=global_step,",
			"\t\t\tlearning_rate=params.learning_rate,",
			"\t\t\toptimizer=params.optimizer,",
			"\t\t\tlearning_rate_decay_fn=decay)",
			"",
			"\treturn tf.contrib.learn.ModelFnOps(",
			"\t\tmode=mode,",
			"\t\tpredictions=predictions,",
			"\t\tloss=loss,",
			"\t\ttrain_op=train_op)",
			"",
			"  return _model_fn",
			"",
			"def experiment_fn(run_config, hparams):",
			"\t\"\"\"Constructs an experiment object.\"\"\"",
			"\testimator = learn.Estimator(",
			"\t\tmodel_fn=make_model_fn(), config=run_config, params=hparams)",
			"\teval_metrics = MODELS[FLAGS.model].eval_metrics_fn(hparams)",
			"\treturn learn.Experiment(",
			"\t\testimator=estimator,",
			"\t\ttrain_input_fn=make_input_fn(learn.ModeKeys.TRAIN, hparams),",
			"\t\teval_input_fn=make_input_fn(learn.ModeKeys.EVAL, hparams),",
			"\t\teval_metrics=eval_metrics,",
			"\t\teval_steps=FLAGS.eval_steps,",
			"\t\tmin_eval_frequency=FLAGS.eval_frequency)",
			"",
			"def main(unused_argv):",
			"\t\"\"\"Main entry point.\"\"\"",
			"\tif FLAGS.output_dir:",
			"\t\tmodel_dir = FLAGS.output_dir",
			"\telse:",
			"\t\tmodel_dir = \"output/%s_%s\" % (FLAGS.model, FLAGS.dataset)",
			"",
			"\tdataset = DATASETS[FLAGS.dataset].prepare_fn()",
			"",
			"\tsession_config = tf.ConfigProto()",
			"\tsession_config.allow_soft_placement = True",
			"\tsession_config.gpu_options.allow_growth = True",
			"\trun_config = learn.RunConfig(",
			"\t\tmodel_dir=model_dir,",
			"\t\tsave_summary_steps=FLAGS.save_summary_steps,",
			"\t\tsave_checkpoints_steps=FLAGS.save_checkpoints_steps,",
			"\t\tsave_checkpoints_secs=None,",
			"\t\tsession_config=session_config)",
			"",
			"\testimator = learn.learn_runner.run(",
			"\t\texperiment_fn=experiment_fn,",
			"\t\trun_config=run_config,",
			"\t\tschedule=FLAGS.schedule,",
			"\t\thparams=get_hparams())",
			"",
			"if __name__ == \"__main__\":",
			"  tf.app.run()",
			""
		],
		"description": "A trainer module based on tf.contrib.learn API."
	},
	"MNIST dataset" : {
		"prefix": "tf:mnist",
		"body": [
			"\"\"\"Mnist dataset preprocessing and specifications.\"\"\"",
			"",
			"from __future__ import absolute_import",
			"from __future__ import division",
			"from __future__ import print_function",
			"",
			"import gzip",
			"import numpy as np",
			"import os",
			"from six.moves import urllib",
			"import struct",
			"import tensorflow as tf",
			"",
			"REMOTE_URL = \"http://yann.lecun.com/exdb/mnist/\"",
			"LOCAL_DIR = \"data/mnist/\"",
			"TRAIN_IMAGE_URL = \"train-images-idx3-ubyte.gz\"",
			"TRAIN_LABEL_URL = \"train-labels-idx1-ubyte.gz\"",
			"TEST_IMAGE_URL = \"t10k-images-idx3-ubyte.gz\"",
			"TEST_LABEL_URL = \"t10k-labels-idx1-ubyte.gz\"",
			"",
			"IMAGE_SIZE = 28",
			"NUM_CLASSES = 10",
			"",
			"HPARAMS = {",
			"\t\"num_classes\": NUM_CLASSES,",
			"}",
			"",
			"def prepare_fn():",
			"\t\"\"\"This function will be called once to prepare the dataset.\"\"\"",
			"\tif not os.path.exists(LOCAL_DIR):",
			"\t\tos.makedirs(LOCAL_DIR)",
			"\tfor name in [",
			"\t\tTRAIN_IMAGE_URL,",
			"\t\tTRAIN_LABEL_URL,",
			"\t\tTEST_IMAGE_URL,",
			"\t\tTEST_LABEL_URL]:",
			"\t\tif not os.path.exists(LOCAL_DIR + name):",
			"\t\t\turllib.request.urlretrieve(REMOTE_URL + name, LOCAL_DIR + name)",
			"",
			"def read_fn(split):",
			"\t\"\"\"Create an instance of the dataset object.\"\"\"",
			"\timage_urls = {",
			"\t\ttf.estimator.ModeKeys.TRAIN: TRAIN_IMAGE_URL,",
			"\t\ttf.estimator.ModeKeys.EVAL: TEST_IMAGE_URL",
			"\t}[split]",
			"\tlabel_urls = {",
			"\t\ttf.estimator.ModeKeys.TRAIN: TRAIN_LABEL_URL,",
			"\t\ttf.estimator.ModeKeys.EVAL: TEST_LABEL_URL",
			"\t}[split]",
			"",
			"\twith gzip.open(LOCAL_DIR + image_urls, \"rb\") as f:",
			"\t\tmagic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))",
			"\t\timages = np.frombuffer(f.read(num * rows * cols), dtype=np.uint8)",
			"\t\timages = np.reshape(images, [num, rows, cols, 1])",
			"\t\tprint(\"Loaded %d images of size [%d, %d].\" % (num, rows, cols))",
			"",
			"\twith gzip.open(LOCAL_DIR + label_urls, \"rb\") as f:",
			"\t\tmagic, num = struct.unpack(\">II\", f.read(8))",
			"\t\tlabels = np.frombuffer(f.read(num), dtype=np.int8)",
			"\t\tprint(\"Loaded %d labels.\" % num)",
			"",
			"\treturn tf.contrib.data.Dataset.from_tensor_slices((images, labels))",
			"",
			"def parser_fn(image, label):",
			"\t\"\"\"Parse input record to features and labels.\"\"\"",
			"\timage = tf.to_float(image) / 255.0",
			"\tlabel = tf.to_int64(label)",
			"\treturn {\"image\": image}, {\"label\": label}",
			""
		],
		"description": "MNIST dataset preprocessing and specifications"
	},
	"CNN classifier": {
		"prefix": "tf:cnn-classifier",
		"body": [
			"\"\"\"Simple convolutional neural network classififer.\"\"\"",
			"",
			"from __future__ import absolute_import",
			"from __future__ import division",
			"from __future__ import print_function",
			"",
			"import numpy as np",
			"import tensorflow as tf",
			"",
			"FLAGS = tf.flags.FLAGS",
			"",
			"HPARAMS = {",
			"\t\"drop_rate\": 0.5",
			"}",
			"",
			"def model_fn(features, labels, mode, params):",
			"\t\"\"\"CNN classifier model.\"\"\"",
			"\timages = features[\"image\"]",
			"\tlabels = labels[\"label\"]",
			"",
			"\ttf.summary.image(\"images\", images)",
			"",
			"\tdrop_rate = params.drop_rate if mode == tf.estimator.ModeKeys.TRAIN else 0.0",
			"",
			"\tfeatures = images",
			"\tfor i, fs in enumerate([32, 64, 128]):",
			"\t\tfeatures = tf.layers.conv2d(",
			"\t\t\tfeatures, filters=fs, kernel_size=3, padding=\"same\",",
			"\t\t\tname=\"conv_%d\" % (i + 1))",
			"\t\tfeatures = tf.layers.max_pooling2d(",
			"\t\t\tinputs=features, pool_size=2, strides=2, padding=\"same\",",
			"\t\t\tname=\"pool_%d\" % (i + 1))",
			"",
			"\tfeatures = tf.contrib.layers.flatten(features)",
			"",
			"\tfeatures = tf.layers.dropout(features, params.drop_rate)",
			"\tfeatures = tf.layers.dense(features, 512, name=\"dense_1\")",
			"",
			"\tfeatures = tf.layers.dropout(features, params.drop_rate)",
			"\tlogits = tf.layers.dense(features, params.num_classes, activation=None,",
			"\t\t\t\t\t\t\t name=\"dense_2\")",
			"",
			"\tpredictions = tf.argmax(logits, axis=1)",
			"",
			"\tloss = tf.losses.sparse_softmax_cross_entropy(",
			"\t\tlabels=labels, logits=logits)",
			"",
			"\treturn {\"predictions\": predictions}, loss",
			"",
			"def eval_metrics_fn(params):",
			"\t\"\"\"Eval metrics.\"\"\"",
			"\treturn {",
			"\t\t\"accuracy\": tf.contrib.learn.MetricSpec(tf.metrics.accuracy)",
			"\t}",
			""
		],
		"description": "A simple convolutional neural network classifier."
	}
}
